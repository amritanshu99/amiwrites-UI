🧠 1. Why RL Instead of Just Counting?

If you just count clicks → old blogs always dominate.

If you just count impressions → noisy and unfair.

RL solves this: balance explore vs exploit.

👉 Exploit = keep showing blogs with proven high CTR.
👉 Explore = still test less-known blogs to see if they can surprise.

This is exactly what your Thompson Sampling does.

📦 2. Data Model: BlogStat

Each blog has a stat row in MongoDB (BlogStat.js):

{
  postId: ObjectId,         // link to Blog
  alpha: 8.14,              // engaged reads (successes)
  beta: 33.31,              // unengaged reads (failures)
  impressions: 127.05,      // times blog was shown
  clicks: 23.54,            // times user clicked from list
  engaged_count: 6.85,      // engaged reads count
  words: 606,               // blog length (for read time calc)
  category: null,
  publishedAt: 2025-08-23,  // when blog published
  lastUpdated: 2025-09-02,  // last event processed
}


💡 Key Idea:

alpha & beta = the “brain” → belief about quality.

impressions/clicks = support metrics for CTR monitoring.

engaged_count = debugging/analytics.

words = used to compute expected reading time.

⚡ 3. API Lifecycle
(a) Impression API (/events/impression)

📍 When called:

On blog detail page load (first render).

📍 What happens in DB:

$inc: { impressions: 1 }


📍 Meaning:

"This blog was seen once."

No reward signal yet (user may bounce).

(b) Click API (/events/click)

📍 When called:

When user clicks a blog from list/grid view.

📍 What happens in DB:

$inc: { clicks: 1 }


📍 Meaning:

"User chose this blog over others."

Still no α/β change (that happens only at read-end).

(c) Read-End API (/events/read-end)

📍 When called:

On scroll/unload/visibilitychange after user opened blog.

Also snapshot at 10s even if user doesn’t leave.

📍 What happens:

Compute expected dwell time:

expected_ms = words / 200 * 60 * 1000


606 words → ~3 min expected.

Compute actual dwell ratio:

ratio = dwell_ms / expected_ms


If dwell ≥ 60% expected OR scroll ≥ 70% → engaged = true.

Update BlogStat:

if engaged:
  alpha += 1
  engaged_count += 1
else:
  beta += 1


lastUpdated set to now.

📍 Meaning:

This is the reward feedback.

Every read is either a success (alpha) or failure (beta).

This directly trains RL.

🎲 4. Thompson Sampling (The Brain)

In /trending API:

theta = betaSample(alpha, beta)
score = theta * freshBoost * jitter


betaSample(α, β) = draw random value from Beta distribution.

Blogs with high α/low β → θ drawn close to 1.

Blogs with low α/high β → θ drawn close to 0.

But randomness = exploration: even weak blogs get rare high draws.

📍 Boosts added:

freshBoost = 1.1 → if blog <72h old, small push.

jitter = ±1% → break ties, add randomness.

📍 Sorting:

Blogs ranked by score.

Top N (limit=4 by default, can be 2/12 via query) returned.

⏳ 5. Decay Mechanism

In utils/decay.js:
Every day at 03:15 AM, run:

$mul: {
  alpha: 0.97,
  beta: 0.97,
  impressions: 0.97,
  clicks: 0.97,
  engaged_count: 0.97
}


📍 Effect:

Old stats fade by 3% daily.

Recent behavior dominates.

Prevents old “superstars” from staying top forever.

🕓 6. Trending API (/trending)

Steps inside getTrending:

Time filter:

Default = last 60 days (windowDays).

Override with ?all=1 to bypass.

Fetch posts from Blog collection.

Match stats from BlogStat.

Compute score for each:

θ (random from Beta).

Apply freshBoost if <72h old.

Apply small jitter.

Sort descending by score.

Pick unique categories first (diversity).

Fill remaining slots.

Ensure 1 fresh blog if possible.

Return as items.

🔁 7. Walkthrough Example

Suppose you have 3 blogs:

Blog A (old, strong)

α=80, β=20 → CTR ≈ 80%.

θ usually ≈ 0.8.

Blog B (new, untested)

α=2, β=3 → CTR ≈ 40%, but uncertain.

θ sometimes 0.7, sometimes 0.2.

Gets freshBoost (10%).

Blog C (bad)

α=1, β=50 → CTR ≈ 2%.

θ rarely >0.1.

👉 Trending picks:

Usually Blog A.

Sometimes Blog B wins lottery → exposure.

Blog C almost never, but tiny chance.

📊 8. What Happens if User Refreshes?

Impression API fires every time → impressions++.

Click API fires if clicked from list → clicks++.

Read-End API fires on exit → α++ or β++.

If refresh = bot-like (<5s, no scroll) → ignored.

Effect:

Repeated refreshes inflate impressions.

If no engagement, beta rises → blog trends less.

If engaged reads, alpha rises → blog trends more.

🧾 9. Debugging Checklist

Check BlogStat:

db.blogstats.find({postId:ObjectId("...")}).pretty()


α/β: trending quality score.

impressions/clicks: exposure counts.

engaged_count: successes.

lastUpdated: freshness.

Force blog trending:

db.blogstats.updateOne(
  {postId:ObjectId("...")},
  {$set:{alpha:100, beta:1}}
)


Guarantees high θ draws.

Simulate decay:

Check if α/β are shrinking daily.

🔍 10. Edge Cases

No blogs in DB → trending returns [].

Less blogs than limit → trending returns all available.

New blogs with no reads → α=1.5, β=1 (priors).

Bot-like readers (<5s + <15% scroll) → ignored.

Blogs older than 60 days → excluded unless ?all=1.

🎯 11. Why This Is RL in Action

Agent = trending API.

Actions = which blogs to display.

Environment = users.

Reward = engagement (alpha increments).

Policy = Thompson Sampling with priors + decay.

Exploration = randomness in θ.

Exploitation = blogs with high α trending more.

👉 Over time, the system learns automatically which blogs are good and keeps adapting.

✅ In One Line:
Your system is a self-updating bandit where every user event teaches the algorithm — impressions track exposure, clicks show interest, read-end decides quality, decay forgets old data, and Thompson Sampling balances randomness with confidence to choose which blogs trend.

🔥 Now you can:

Debug by checking α/β.

Manually push/pull a blog into trending.

Adjust exploration by changing priors or decay.

Control time window via windowDays.









📌 Where alpha & beta live

They are stored in BlogStat model:

alpha: { type: Number, default: 1.5 }, // Thompson Sampling priors
beta:  { type: Number, default: 1.0 },

📌 What they mean

alpha = success counter → counts “good/engaged interactions” (like positive signals).

beta = failure counter → counts “bad/neutral interactions” (like skipped or unengaged).

Together, they form a Beta distribution → used by Thompson Sampling to estimate the probability that this blog is engaging.

📌 How they change in your code
1. On Impression
await BlogStat.updateOne(
  { postId },
  {
    $setOnInsert: {
      alpha: PRIORS.alpha, // = 1.5
      beta: PRIORS.beta,   // = 1.0
    },
    $inc: { impressions: 1 }
  },
  { upsert: true }
);


📌 Only impressions increases.

alpha and beta stay the same.

2. On Click
await BlogStat.updateOne(
  { postId },
  {
    $setOnInsert: {
      alpha: PRIORS.alpha,
      beta: PRIORS.beta,
    },
    $inc: { clicks: 1 }
  },
  { upsert: true }
);


📌 Only clicks increases.

alpha and beta stay the same.

3. On Read-End (important)
const engaged =
  ratio >= 0.6 ||  // read enough time
  scroll >= 0.7 || // scrolled enough
  !!bookmarked ||  // saved
  !!shared;        // shared

const inc = { engaged_count: engaged ? 1 : 0 };
if (engaged) inc.alpha = 1;  // success
else inc.beta = 1;           // failure


If user engaged →
alpha = alpha + 1 ✅

If user not engaged →
beta = beta + 1 ❌

4. On Decay Job

In /utils/decay.js:

await BlogStat.updateMany({}, {
  $mul: {
    alpha: DECAY,
    beta: DECAY,
    impressions: DECAY,
    clicks: DECAY,
    engaged_count: DECAY,
  }
});


📌 Every night, both alpha & beta are shrunk by 0.97 (or whatever DECAY is set to).

This means old stats fade away over time so recent behavior matters more.

📌 Example

Blog starts:

alpha = 1.5
beta = 1.0


Gets a read where user scrolls deep + spends time:

alpha = 2.5
beta = 1.0


Next user bounces quickly:

alpha = 2.5
beta = 2.0


Decay runs overnight (DECAY = 0.97):

alpha = 2.425
beta = 1.94


👉 So the blog’s “strength” is always shifting based on real engagement + fading memory.

✅ Summary

alpha ↑ = more engaged reads (success).

beta ↑ = more non-engaged reads (failure).

They don’t depend on impressions or clicks directly → only read-end engagement updates them.

Decay ensures recent behavior matters more than old behavior.




📌 Where engagement is calculated

In controllers/trendingRLController.js → trackReadEnd:

const words = blog.words || wordsFromBlog(blog);
const expected = computeExpectedMs(words);   // ⬅️ here
const dwell = Number(dwell_ms) || 0;
const scroll = Number(scroll_depth);
const ratio = expected > 0 ? dwell / expected : 0;

const engaged =
  ratio >= 0.6 ||         // spent enough time
  (Number.isFinite(scroll) && scroll >= 0.7) ||  // scrolled enough
  !!bookmarked ||
  !!shared;

📌 How expected is computed

See the helper above:

function computeExpectedMs(words = 0) {
  return (Math.max(50, words) / 200) * 60 * 1000;
}


Assume an average reading speed = 200 words/minute.

So for each blog, expected read time =

(words / 200) * 60 seconds * 1000 → milliseconds


Floor to 50 words → to avoid division by zero for tiny blogs.

📌 Example calculation

Suppose blog has 600 words (like your BlogStat row):

expected = (600 / 200) * 60 * 1000
         = 3 * 60 * 1000
         = 180,000 ms


So expected_ms = 180 seconds (~3 min).

📌 Engagement rules

If dwell_ms / expected_ms ≥ 0.6 → Engaged ✅

Example: user stays 120s on a 180s blog → ratio = 0.66 → engaged.

Or if scroll_depth ≥ 0.7 → Engaged ✅

Example: user scrolls 80% of page → engaged even if time was short.

Or if bookmarked/shared → Engaged ✅

Else → Not engaged ❌

📌 What gets stored

If engaged → alpha++
Else → beta++

The actual expected_ms is not saved anywhere — it’s recomputed each time using blog’s word count.

✅ Summary

expected_ms is not a DB field.

It’s computed dynamically based on blog word count.

Engagement is then judged by comparing dwell_ms (real user time) vs expected_ms (predicted reading time).

This way, long blogs don’t get unfairly punished (because they get more expected time).