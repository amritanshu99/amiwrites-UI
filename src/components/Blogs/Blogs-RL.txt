ğŸ§  1. Why RL Instead of Just Counting?

If you just count clicks â†’ old blogs always dominate.

If you just count impressions â†’ noisy and unfair.

RL solves this: balance explore vs exploit.

ğŸ‘‰ Exploit = keep showing blogs with proven high CTR.
ğŸ‘‰ Explore = still test less-known blogs to see if they can surprise.

This is exactly what your Thompson Sampling does.

ğŸ“¦ 2. Data Model: BlogStat

Each blog has a stat row in MongoDB (BlogStat.js):

{
  postId: ObjectId,         // link to Blog
  alpha: 8.14,              // engaged reads (successes)
  beta: 33.31,              // unengaged reads (failures)
  impressions: 127.05,      // times blog was shown
  clicks: 23.54,            // times user clicked from list
  engaged_count: 6.85,      // engaged reads count
  words: 606,               // blog length (for read time calc)
  category: null,
  publishedAt: 2025-08-23,  // when blog published
  lastUpdated: 2025-09-02,  // last event processed
}


ğŸ’¡ Key Idea:

alpha & beta = the â€œbrainâ€ â†’ belief about quality.

impressions/clicks = support metrics for CTR monitoring.

engaged_count = debugging/analytics.

words = used to compute expected reading time.

âš¡ 3. API Lifecycle
(a) Impression API (/events/impression)

ğŸ“ When called:

On blog detail page load (first render).

ğŸ“ What happens in DB:

$inc: { impressions: 1 }


ğŸ“ Meaning:

"This blog was seen once."

No reward signal yet (user may bounce).

(b) Click API (/events/click)

ğŸ“ When called:

When user clicks a blog from list/grid view.

ğŸ“ What happens in DB:

$inc: { clicks: 1 }


ğŸ“ Meaning:

"User chose this blog over others."

Still no Î±/Î² change (that happens only at read-end).

(c) Read-End API (/events/read-end)

ğŸ“ When called:

On scroll/unload/visibilitychange after user opened blog.

Also snapshot at 10s even if user doesnâ€™t leave.

ğŸ“ What happens:

Compute expected dwell time:

expected_ms = words / 200 * 60 * 1000


606 words â†’ ~3 min expected.

Compute actual dwell ratio:

ratio = dwell_ms / expected_ms


If dwell â‰¥ 60% expected OR scroll â‰¥ 70% â†’ engaged = true.

Update BlogStat:

if engaged:
  alpha += 1
  engaged_count += 1
else:
  beta += 1


lastUpdated set to now.

ğŸ“ Meaning:

This is the reward feedback.

Every read is either a success (alpha) or failure (beta).

This directly trains RL.

ğŸ² 4. Thompson Sampling (The Brain)

In /trending API:

theta = betaSample(alpha, beta)
score = theta * freshBoost * jitter


betaSample(Î±, Î²) = draw random value from Beta distribution.

Blogs with high Î±/low Î² â†’ Î¸ drawn close to 1.

Blogs with low Î±/high Î² â†’ Î¸ drawn close to 0.

But randomness = exploration: even weak blogs get rare high draws.

ğŸ“ Boosts added:

freshBoost = 1.1 â†’ if blog <72h old, small push.

jitter = Â±1% â†’ break ties, add randomness.

ğŸ“ Sorting:

Blogs ranked by score.

Top N (limit=4 by default, can be 2/12 via query) returned.

â³ 5. Decay Mechanism

In utils/decay.js:
Every day at 03:15 AM, run:

$mul: {
  alpha: 0.97,
  beta: 0.97,
  impressions: 0.97,
  clicks: 0.97,
  engaged_count: 0.97
}


ğŸ“ Effect:

Old stats fade by 3% daily.

Recent behavior dominates.

Prevents old â€œsuperstarsâ€ from staying top forever.

ğŸ•“ 6. Trending API (/trending)

Steps inside getTrending:

Time filter:

Default = last 60 days (windowDays).

Override with ?all=1 to bypass.

Fetch posts from Blog collection.

Match stats from BlogStat.

Compute score for each:

Î¸ (random from Beta).

Apply freshBoost if <72h old.

Apply small jitter.

Sort descending by score.

Pick unique categories first (diversity).

Fill remaining slots.

Ensure 1 fresh blog if possible.

Return as items.

ğŸ” 7. Walkthrough Example

Suppose you have 3 blogs:

Blog A (old, strong)

Î±=80, Î²=20 â†’ CTR â‰ˆ 80%.

Î¸ usually â‰ˆ 0.8.

Blog B (new, untested)

Î±=2, Î²=3 â†’ CTR â‰ˆ 40%, but uncertain.

Î¸ sometimes 0.7, sometimes 0.2.

Gets freshBoost (10%).

Blog C (bad)

Î±=1, Î²=50 â†’ CTR â‰ˆ 2%.

Î¸ rarely >0.1.

ğŸ‘‰ Trending picks:

Usually Blog A.

Sometimes Blog B wins lottery â†’ exposure.

Blog C almost never, but tiny chance.

ğŸ“Š 8. What Happens if User Refreshes?

Impression API fires every time â†’ impressions++.

Click API fires if clicked from list â†’ clicks++.

Read-End API fires on exit â†’ Î±++ or Î²++.

If refresh = bot-like (<5s, no scroll) â†’ ignored.

Effect:

Repeated refreshes inflate impressions.

If no engagement, beta rises â†’ blog trends less.

If engaged reads, alpha rises â†’ blog trends more.

ğŸ§¾ 9. Debugging Checklist

Check BlogStat:

db.blogstats.find({postId:ObjectId("...")}).pretty()


Î±/Î²: trending quality score.

impressions/clicks: exposure counts.

engaged_count: successes.

lastUpdated: freshness.

Force blog trending:

db.blogstats.updateOne(
  {postId:ObjectId("...")},
  {$set:{alpha:100, beta:1}}
)


Guarantees high Î¸ draws.

Simulate decay:

Check if Î±/Î² are shrinking daily.

ğŸ” 10. Edge Cases

No blogs in DB â†’ trending returns [].

Less blogs than limit â†’ trending returns all available.

New blogs with no reads â†’ Î±=1.5, Î²=1 (priors).

Bot-like readers (<5s + <15% scroll) â†’ ignored.

Blogs older than 60 days â†’ excluded unless ?all=1.

ğŸ¯ 11. Why This Is RL in Action

Agent = trending API.

Actions = which blogs to display.

Environment = users.

Reward = engagement (alpha increments).

Policy = Thompson Sampling with priors + decay.

Exploration = randomness in Î¸.

Exploitation = blogs with high Î± trending more.

ğŸ‘‰ Over time, the system learns automatically which blogs are good and keeps adapting.

âœ… In One Line:
Your system is a self-updating bandit where every user event teaches the algorithm â€” impressions track exposure, clicks show interest, read-end decides quality, decay forgets old data, and Thompson Sampling balances randomness with confidence to choose which blogs trend.

ğŸ”¥ Now you can:

Debug by checking Î±/Î².

Manually push/pull a blog into trending.

Adjust exploration by changing priors or decay.

Control time window via windowDays.









ğŸ“Œ Where alpha & beta live

They are stored in BlogStat model:

alpha: { type: Number, default: 1.5 }, // Thompson Sampling priors
beta:  { type: Number, default: 1.0 },

ğŸ“Œ What they mean

alpha = success counter â†’ counts â€œgood/engaged interactionsâ€ (like positive signals).

beta = failure counter â†’ counts â€œbad/neutral interactionsâ€ (like skipped or unengaged).

Together, they form a Beta distribution â†’ used by Thompson Sampling to estimate the probability that this blog is engaging.

ğŸ“Œ How they change in your code
1. On Impression
await BlogStat.updateOne(
  { postId },
  {
    $setOnInsert: {
      alpha: PRIORS.alpha, // = 1.5
      beta: PRIORS.beta,   // = 1.0
    },
    $inc: { impressions: 1 }
  },
  { upsert: true }
);


ğŸ“Œ Only impressions increases.

alpha and beta stay the same.

2. On Click
await BlogStat.updateOne(
  { postId },
  {
    $setOnInsert: {
      alpha: PRIORS.alpha,
      beta: PRIORS.beta,
    },
    $inc: { clicks: 1 }
  },
  { upsert: true }
);


ğŸ“Œ Only clicks increases.

alpha and beta stay the same.

3. On Read-End (important)
const engaged =
  ratio >= 0.6 ||  // read enough time
  scroll >= 0.7 || // scrolled enough
  !!bookmarked ||  // saved
  !!shared;        // shared

const inc = { engaged_count: engaged ? 1 : 0 };
if (engaged) inc.alpha = 1;  // success
else inc.beta = 1;           // failure


If user engaged â†’
alpha = alpha + 1 âœ…

If user not engaged â†’
beta = beta + 1 âŒ

4. On Decay Job

In /utils/decay.js:

await BlogStat.updateMany({}, {
  $mul: {
    alpha: DECAY,
    beta: DECAY,
    impressions: DECAY,
    clicks: DECAY,
    engaged_count: DECAY,
  }
});


ğŸ“Œ Every night, both alpha & beta are shrunk by 0.97 (or whatever DECAY is set to).

This means old stats fade away over time so recent behavior matters more.

ğŸ“Œ Example

Blog starts:

alpha = 1.5
beta = 1.0


Gets a read where user scrolls deep + spends time:

alpha = 2.5
beta = 1.0


Next user bounces quickly:

alpha = 2.5
beta = 2.0


Decay runs overnight (DECAY = 0.97):

alpha = 2.425
beta = 1.94


ğŸ‘‰ So the blogâ€™s â€œstrengthâ€ is always shifting based on real engagement + fading memory.

âœ… Summary

alpha â†‘ = more engaged reads (success).

beta â†‘ = more non-engaged reads (failure).

They donâ€™t depend on impressions or clicks directly â†’ only read-end engagement updates them.

Decay ensures recent behavior matters more than old behavior.




ğŸ“Œ Where engagement is calculated

In controllers/trendingRLController.js â†’ trackReadEnd:

const words = blog.words || wordsFromBlog(blog);
const expected = computeExpectedMs(words);   // â¬…ï¸ here
const dwell = Number(dwell_ms) || 0;
const scroll = Number(scroll_depth);
const ratio = expected > 0 ? dwell / expected : 0;

const engaged =
  ratio >= 0.6 ||         // spent enough time
  (Number.isFinite(scroll) && scroll >= 0.7) ||  // scrolled enough
  !!bookmarked ||
  !!shared;

ğŸ“Œ How expected is computed

See the helper above:

function computeExpectedMs(words = 0) {
  return (Math.max(50, words) / 200) * 60 * 1000;
}


Assume an average reading speed = 200 words/minute.

So for each blog, expected read time =

(words / 200) * 60 seconds * 1000 â†’ milliseconds


Floor to 50 words â†’ to avoid division by zero for tiny blogs.

ğŸ“Œ Example calculation

Suppose blog has 600 words (like your BlogStat row):

expected = (600 / 200) * 60 * 1000
         = 3 * 60 * 1000
         = 180,000 ms


So expected_ms = 180 seconds (~3 min).

ğŸ“Œ Engagement rules

If dwell_ms / expected_ms â‰¥ 0.6 â†’ Engaged âœ…

Example: user stays 120s on a 180s blog â†’ ratio = 0.66 â†’ engaged.

Or if scroll_depth â‰¥ 0.7 â†’ Engaged âœ…

Example: user scrolls 80% of page â†’ engaged even if time was short.

Or if bookmarked/shared â†’ Engaged âœ…

Else â†’ Not engaged âŒ

ğŸ“Œ What gets stored

If engaged â†’ alpha++
Else â†’ beta++

The actual expected_ms is not saved anywhere â€” itâ€™s recomputed each time using blogâ€™s word count.

âœ… Summary

expected_ms is not a DB field.

Itâ€™s computed dynamically based on blog word count.

Engagement is then judged by comparing dwell_ms (real user time) vs expected_ms (predicted reading time).

This way, long blogs donâ€™t get unfairly punished (because they get more expected time).